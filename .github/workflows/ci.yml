# GitHub Actions CI/CD Pipeline (Alternative to Azure Pipelines)
# This provides the same testing capabilities for GitHub-hosted projects

name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # Nightly run at 2 AM UTC

env:
  PYTHON_VERSION: '3.11'
  FLASK_ENV: testing
  SECRET_KEY: github-actions-secret-key

jobs:
  # Job 1: Linting and Code Quality
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black bandit
          pip install -r requirements.txt
      
      - name: Lint with flake8
        run: |
          flake8 app.py --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 app.py --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      
      - name: Check code formatting with black
        run: black --check app.py
        continue-on-error: true
      
      - name: Security scan with bandit
        run: bandit -r app.py -f json -o bandit-report.json
        continue-on-error: true
      
      - name: Upload security report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-report
          path: bandit-report.json

  # Job 2: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Initialize database
        run: python -c "import app; app.init_db()"
      
      - name: Run unit tests
        run: pytest tests/unit/ -v --tb=short --junitxml=unit-test-results.xml
        continue-on-error: true
      
      - name: Publish unit test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Unit Tests
          path: unit-test-results.xml
          reporter: java-junit

  # Job 3: Functional API Tests
  api-tests:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run API functional tests
        run: pytest tests/functional/test_api_endpoints.py -v --tb=short --junitxml=api-test-results.xml
      
      - name: Publish API test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: API Functional Tests
          path: api-test-results.xml
          reporter: java-junit

  # Job 4: UI Tests with Selenium
  ui-tests:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest
      
      - name: Install ChromeDriver
        uses: nanasess/setup-chromedriver@master
      
      - name: Start Xvfb
        run: |
          sudo apt-get install -y xvfb
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
      
      - name: Run UI tests
        run: |
          export DISPLAY=:99
          pytest tests/functional/test_ui_selenium.py -v --tb=short --junitxml=ui-test-results.xml
        env:
          DISPLAY: ':99'
      
      - name: Publish UI test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: UI Functional Tests
          path: ui-test-results.xml
          reporter: java-junit

  # Job 5: Security Tests
  security-tests:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Chrome for security tests
        uses: browser-actions/setup-chrome@latest
      
      - name: Start Xvfb
        run: |
          sudo apt-get install -y xvfb
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
      
      - name: Run security tests
        run: |
          export DISPLAY=:99
          pytest tests/non_functional/test_security.py -v --tb=short --junitxml=security-test-results.xml
        env:
          DISPLAY: ':99'
      
      - name: Publish security test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Security Tests
          path: security-test-results.xml
          reporter: java-junit

  # Job 6: Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run performance tests
        run: pytest tests/non_functional/test_performance.py -v --tb=short --junitxml=performance-test-results.xml
      
      - name: Publish performance test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Performance Tests
          path: performance-test-results.xml
          reporter: java-junit

  # Job 7: Load Testing
  load-tests:
    runs-on: ubuntu-latest
    needs: [api-tests]
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run load tests
        run: |
          # Start the application in background
          python app.py &
          APP_PID=$!
          sleep 10
          
          # Run load test
          locust -f tests/non_functional/test_load_locust.py --host=http://localhost:5000 --headless -u 10 -r 2 -t 60s --html load-test-report.html
          
          # Stop the application
          kill $APP_PID
      
      - name: Upload load test report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: load-test-report
          path: load-test-report.html

  # Job 8: Docker Build and Test
  docker-tests:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker image
        run: |
          docker build -t flask-auth-system:${{ github.sha }} --target app .
          docker build -t flask-auth-system-test:${{ github.sha }} --target ci .
      
      - name: Test Docker container
        run: |
          # Start container
          docker run -d --name test-app -p 5000:5000 flask-auth-system:${{ github.sha }}
          sleep 10
          
          # Health check
          curl -f http://localhost:5000/api/health
          
          # Stop container
          docker stop test-app
          docker rm test-app
      
      - name: Run tests in Docker
        run: |
          mkdir -p docker-test-results
          docker run --rm -v $(pwd)/docker-test-results:/app/test-results flask-auth-system-test:${{ github.sha }}
      
      - name: Publish Docker test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Docker Tests
          path: docker-test-results/junit/test-results.xml
          reporter: java-junit
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: docker-test-results/coverage.xml
          flags: docker-tests

  # Job 9: Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [api-tests, ui-tests, security-tests, performance-tests]
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest
      
      - name: Start Xvfb
        run: |
          sudo apt-get install -y xvfb
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
      
      - name: Run integration tests
        run: |
          export DISPLAY=:99
          pytest tests/ -m "not slow" -v --tb=short --junitxml=integration-test-results.xml --cov=app --cov-report=xml:coverage.xml
        env:
          DISPLAY: ':99'
      
      - name: Publish integration test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Integration Tests
          path: integration-test-results.xml
          reporter: java-junit
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: coverage.xml
          flags: integration-tests

  # Job 10: Deployment Readiness Check
  deployment-check:
    runs-on: ubuntu-latest
    needs: [integration-tests, docker-tests]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      
      - name: Deployment readiness check
        run: |
          echo "All tests passed! Application is ready for deployment."
          echo "Build SHA: ${{ github.sha }}"
          echo "Build URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      
      - name: Create deployment artifact
        run: |
          mkdir -p deployment
          echo "BUILD_SHA=${{ github.sha }}" > deployment/build.env
          echo "BUILD_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> deployment/build.env
          echo "TESTS_PASSED=true" >> deployment/build.env
      
      - name: Upload deployment artifact
        uses: actions/upload-artifact@v3
        with:
          name: deployment-info
          path: deployment/
